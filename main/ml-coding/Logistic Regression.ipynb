{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e210c7b6",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression\n",
    "In this note, we will implement the logistic regression algorithm for binary classification. \n",
    "\n",
    "Let $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\ldots, (x^{(n)}, y^{(n)})$ be a sequence of training samples, where $x^{(i)} \\in \\mathbb{R}^d$, and $y^{(i)} \\in \\{0, 1\\}$ for $i = 1, \\ldots, n$. Let\n",
    "$$X := \\begin{pmatrix} x^{(1)^T} \\\\ \\vdots \\\\  x^{(n)^T} \\end{pmatrix}, y := \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\  y^{(n)} \\end{pmatrix}.$$\n",
    "\n",
    "Our goal in binary classification is to train a model $h$ that will predict the label of a new sample $x$. We often use the following model:\n",
    "$$h_{\\theta}(x) = g(x^T \\theta),$$\n",
    "where\n",
    "$$ \\theta := \\begin{pmatrix} \\theta_{0} \\\\ \\vdots \\\\  \\theta_{d} \\end{pmatrix}, $$ \n",
    "and\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "is called the **logistic function** or the **sigmoid function**. We have also used the convention that $x^T \\theta = \\theta_0 + \\sum_{i = 1}^{d}\\theta_i x_i$. Note that $g \\in (0, 1)$, which serves as a good choice as far as binary classification is concerned. In order to train our model, we try to maximize the log-likelihood function given as follows:\n",
    "$$\\ell(\\theta) = \\log L(\\theta) = \\sum_{i = 1}^{n}y^{(i)}\\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})\\log(1 - h_{\\theta}(x^{(i)}))$$.\n",
    "\n",
    "## 1.1 Gradient Ascent\n",
    "\n",
    "There are several methods to maximize $\\ell(\\theta)$. We will discuss some of the more popular ones here. The first is by using the *gradient ascent* method. That is, our updates to $\\theta$ will be given by $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}\\ell(\\theta)$, where $\\alpha$ is the so-called *learning rate*, and $\\nabla$ denotes the gradient. By taking the gradient of $\\ell(\\theta)$ and using the fact that $g'(z) = g(z)(1 - g(z))$, we arrive at $\\theta \\leftarrow \\theta - \\alpha X^T (h_{\\theta}(X) - y)$, where $h_{\\theta}(X)$ is understood in the vector form. Below, you can find an implementation of the gradient ascent approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74bc1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed34c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate = 0.01, iterations = 1000):\n",
    "        self.theta = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, d_features = X.shape\n",
    "        X = np.hstack([np.ones((n_samples, 1)), X])\n",
    "        self.theta = np.zeros(d_features + 1)\n",
    "        alpha = self.learning_rate\n",
    "        for k in range(self.iterations):\n",
    "            z = X @ self.theta\n",
    "            g = X.T @ (self._sigmoid(z) - y)\n",
    "            self.theta = self.theta - alpha * g\n",
    "        \n",
    "        y_pred = self._sigmoid(z)\n",
    "        cost = np.sum(np.log(y_pred)*y + np.log(1 - y_pred)*(1 - y))\n",
    "        print(cost)\n",
    "        return self.theta\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X = np.hstack([np.ones((n_samples, 1)), X])\n",
    "        z = self._sigmoid(X @ self.theta)\n",
    "        return np.round(z).astype(int)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33b3df43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9048467015131948\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# create sample dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# initialize logistic regression model\n",
    "LR = LogisticRegression()\n",
    "\n",
    "# train model on sample dataset\n",
    "LR.fit(X, y)\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = np.array([[6, 7], [7, 8]])\n",
    "y_pred = LR.predict(X_new)\n",
    "\n",
    "print(y_pred)  # [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b517952",
   "metadata": {},
   "source": [
    "## 1.2 Batch Gradient Ascent\n",
    "One caveat of the gradient ascent or descent method in general (not specific to binary classification) is that we use all $n$ training samples to compute the gradient function. Naturally, this could be too costly if $n$ is very large. Another approach that bypasses this issue is the *batch gradient ascent* method, where we only use small batches of the training sample in every iteration in order to compute the gradient. Below, you can find an implementation of the batch gradient ascent approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d89a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16653f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, batch_size = 32, learning_rate = 0.01, iterations = 1000):\n",
    "        self.theta = None\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, d_features = X.shape\n",
    "        X = np.hstack([np.ones((n_samples, 1)), X])\n",
    "        self.theta = np.zeros(d_features + 1)\n",
    "        alpha = self.learning_rate\n",
    "        for k in range(self.iterations):\n",
    "            batch_indices = np.random.choice(n_samples, self.batch_size)\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            z = X_batch @ self.theta\n",
    "            g = X_batch.T @ (self._sigmoid(z) - y_batch)\n",
    "            self.theta = self.theta - alpha * g\n",
    "        \n",
    "        y_pred = self._sigmoid(X @ self.theta)\n",
    "        cost = np.sum(np.log(y_pred)*y + np.log(1 - y_pred)*(1 - y))\n",
    "        print(cost)\n",
    "        return self.theta\n",
    "            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X = np.hstack([np.ones((n_samples, 1)), X])\n",
    "        z = self._sigmoid(X @ self.theta)\n",
    "        return np.round(z).astype(int)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b26a26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.419651602719068\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# create sample dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "# initialize logistic regression model\n",
    "LR = LogisticRegression(batch_size = 2)\n",
    "\n",
    "# train model on sample dataset\n",
    "LR.fit(X, y)\n",
    "\n",
    "# make predictions on new data\n",
    "X_new = np.array([[6, 7], [7, 8]])\n",
    "y_pred = LR.predict(X_new)\n",
    "\n",
    "print(y_pred)  # [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06267b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
